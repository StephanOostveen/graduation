\label{subsec:programmablenode}
As mentioned above, the programmable end nodes use a real-time operating system. From the vendor's documentation\footnote{\url{http://support.openecu.com/doc_user/openecu_user_guide_c_api/openecu_user_guide_c_api.html}, accessed 18 January 2024} we get the following system model. Tasks are implemented as single-shot functions, meaning they run until completion and don't wait. Tasks become ready due to their periodicity, which is handled by the scheduler. The scheduler is a fixed priority preemptive scheduler which itself executes every 1 millisecond. When no tasks are running a background task is scheduled until another tasks becomes ready. The documentation does not specify the behaviour when two tasks have the same priority. Numerically lower tasks have lower priority, i.e. a task with priority 1 has lower priority than a task with priority 2. A description of the tasks defined by the operating system is shown in Table~\ref{tab:priorities}. 

\input{rtos_tasks.tex}

Unfortunately the documentation is a bit unclear in the specification of the tasks, two different CAN tasks with two different priorities are specified, the CAN tasks become ready based on an event and then executes for a number of iterations without specifying the event or the number of iterations. In one place the application tasks are defined to be triggered based on the crank or engine position, this is contradictory to other parts of the documentation where the trigger is defined as a periodic value given by the user. 

Where the documentation was unclear or contradicting the following assumptions are made: the low priority CAN task is responsible for transmitting messages while the high priority task is responsible for reading messages. The reason being that we prefer copying received messages from the hardware peripheral's buffer to a software buffer than transmitting a message and missing a CAN message because the receive buffers were full. The event triggering the receive task is assumed to be the reception of a CAN message in a hardware buffer. Reason being that microcontrollers often have an interrupt for this event. The number of iterations for the receive task is assumed to be 1, the task copies all available messages at that time from the hardware buffer into a software buffer. This is a simple implementation of an interrupt, more complex behaviour is possible but unlikely given the lack of documentation. The CAN transmit task is further defined in the CAN section of the documentation. It is assumed that the ECU has two hardware buffers for each CAN bus, if the application transmits a CAN frame, it first tries the hardware buffer, bypassing the CAN transmit task. If the buffer is full it is stored in the transmit task's software buffer, which triggers the transmit task to become ready. Once the task is scheduled it will put the highest priority buffered frame into arbitration for each CAN bus. If there are more buffered messages the task will be rescheduled after a 2ms cooldown. The documentation states that this is done to "lessen the load experienced by the CAN bus". Finally, the application tasks are assumed to be periodic instead of relative to the engine position.

From a programmers point of view the software is split up in several logical nodes, implementing a related set of functions. E.g the \textit{braking system manager} or the \textit{lighting manager}. The logical nodes can be further split into one or more \textit{runnables}, the runnable is the smallest software component that is subject to scheduling by the RTOS. This allows to split a logical node in several runnables which can be scheduled independently or deployed on different physical nodes. The software implementing the required functions is decoupled from the physical node it is deployed on, allowing functionality to be moved around as necessary. With the help of configuration files the building process knows which runnables should be compiled and linked for which physical node.

Runnables communicate with each other through signals, a signal represents a sample of some system state variable, named data dictionaries by OpenECU. The system state variables can represent a physical value or some abstract system state. A data dictionary is produced by a single runnable, but can be consumed by multiple runnables. For each logical node a set of deployment files describe which runnables exist, which data dictionaries are consumed and produced by each runnable, at what rate the runnables are scheduled and on which \textit{programmable end nodes} they are deployed. Figure~\ref{fig:logical_example} shows an example of two logical nodes, each consisting of several runnables. The \textit{Brake pedal} runnable of the \textit{Braking System} logical produces a \textit{brake pedal applied} data dictionary which is consumed by the \textit{Rear light} runnable of the \textit{Lighting Manager} logical.

\begin{figure}[htb]
    \centering
    \resizebox{0.95\textwidth}{!}{%
        \tikzfig{logical_example}
    }
 \caption{Example of two logical components consisting of several runnables communicating through data dictionaries}
\label{fig:logical_example}
\end{figure}

The deployment files are used when building binaries for the programmable end nodes to automatically generate code to bridge the required data dictionaries between the runnables. First let's consider a pair of runnables deployed on the same programmable end node. One runnable generates a data dictionary while the other is a consumer of that data dictionary. Because the runnables are deployed on the same programmable end nodes no network traffic is required. In this case the data dictionary can be modelled as a shared global variable residing in the programmable end nodes memory which can be accessed in a thread-safe way by both runnables. See Figure~\ref{fig:one_physical} for an example.
\begin{figure}[htb]
    \centering
    \resizebox{0.35\textwidth}{!}{%
        \tikzfig{runnable_single_physical}
    }
    \caption{Two runnables deployed on one programmable end node communicating through a data dictionary}
    \label{fig:one_physical}
\end{figure}

If the producing and consuming runnables are deployed on different \textit{programmable end nodes} which are directly connected to each other, the data dictionary has to be transmitted on the network connecting them. This happens transparently to the functionality developers since the implementation is decoupled from the deployment. Automatically generated runnables for each programmable end node route the data dictionaries to the right nodes. The data dictionaries are transmitted at a fixed rate by a \textit{CAN transmit} runnable. The runnable reads at once all the data dictionaries from the shared global variables, packs them in the predefined CAN frames and schedules them for transmission by the real-time operating system at once. A \textit{CAN receive} runnable is scheduled at a fixed rate which retrieves the last received instance of each registered CAN frame from the real-time operating system and unpacks the frame into the shared global variables at once. If a frame was not received between two executions the real-time operating system returns the last received instance together with an error code indicating that it is an old frame. As the scheduler is preemptive, both these processes can be preempted by higher priority tasks, care should be taken when setting priorities of the runnables. Also note that these runnables are different from the CAN receive and transmit tasks of the operating system, the runnables act as an abstraction hiding the necessary communication. For efficiency reasons data dictionaries that have the same \textit{programmable end node} as destinations are packed in the same CAN message. The definition of the CAN messages is generated automatically at build time and can vary per build. The packing of data dictionaries in a CAN message and the CAN IDs are picked in a non-deterministic way without an optimization criterion, except for minimizing the number of message definitions. Figure~\ref{fig:physical_example} shows an example deployment of two logical nodes on two physical nodes.

\begin{figure}[htb]
    \centering
    \resizebox{\textwidth}{!}{%
        \tikzfig{deployment_example}
    }
 \caption{Example of two logical components consisting of several runnables communicating through data dictionaries}
\label{fig:physical_example}
\end{figure}

Lastly, if the producing and consuming runnables are deployed on different \textit{programmable end nodes} which are not directly connected to each other, the data dictionary must be bridged across networks by the \textit{Central Gateway}. Bridging is implemented by the \textit{CAN transmit} and \textit{CAN receive} tasks and is also generated automatically. The data dictionaries that need to be bridged but are not used by the \textit{Central Gateway} are stored similarly to data dictionaries which are used, but other runnables can't read or write them. An example is shown in Figure~\ref{fig:bridging_example}. This means that some extra delay is introduced in the transmission of bridged data dictionaries by the scheduling of the receive and transmit runnables in the \textit{Central Gateway}.

\begin{figure}[htb]
    \centering
    \resizebox{\textwidth}{!}{%
        \tikzfig{bridging_example}
    }
 \caption{Example of two logical components whose data dictionaries need to be bridged by the Central Gateway.}
\label{fig:bridging_example}
\end{figure}