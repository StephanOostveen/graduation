\section{Discussion}
\label{sec:discussion}
The model generation of the data dictionaries performed well and little processing of the input data was necessary to generate it. The structured nature of the configuration files for the Lightyear build system ensured that there were no gaps in the data. The method overestimates the rate at which data is produced and consumed because it assumes that a data dictionary is read/written every cycle. A runnable could decide to subsample a data dictionary and the method would not detect that case. Accurately detecting those cases is very complex and even impossible as the condition could depend on a run-time variable. From inspecting the source code these cases seem limited. 

More problematic was the model generation for the received and transmitted CAN frames. While the method is capable of finding which c functions call the real-time operating system transmit and receive functions. The call does not directly contain information such as the CAN ID, message size or CAN bus. This data is contained inside the variables passed to the functions. Unfortunately many ways of passing this data were used by Lightyear and the C language is so complex that it can not be parsed easily. Making it unfeasible to automatically extract the CAN id, size and bus in the scope of this project. Recovering this information required considerable manual effort. Once the CAN id and bus was known retrieving the message size, source and destination was again trivial using the structured configuration files used by the build system. The call graph generation fails to detect certain ways of calling the real-time operating system transmit and receive calls. Specifically when the functions are called using the C language function pointer syntax. This method is used in the \textit{safety supervisor core} runnable in the Safety Control Unit. As a result the number of CAN messages of this runnable is underestimated, and we did not get a full overview of the nodes communicating with the SCU. All these issues could be solved by implementing a similar strategy to the data dictionaries.

Decoupling the runnables logic from the physical deployment makes it possible to move the software around different nodes if the need arises. A drawback of the system is that the mapping of data dictionaries to CAN messages is not fixed and in fact is non-deterministic. During the build phase this mapping is created and thus change between software builds. Meaning all three programmable end nodes must be updated at the same time, but more importantly that the timing characteristics of the system will change. Additionally, changes in execution time of tasks transmitting CAN messages will also affect the timing characteristics of the CAN bus and thus the age of data used by runnables. One solution to mitigate the effects could be to use a defined mapping from data dictionaries to CAN messages. This reduces flexibility or requires all data dictionaries to be transmitted to all nodes, which could overload the CAN bus. 

The benchmark focusses on the periodic CAN messages but in reality the vehicle also contains low priority debugging messages. For time reasons these streams where not benchmarked. The XCP, CCP and UDS protocols are used for diagnostics and firmware upgrades. These are protocols build on top of CAN which are used to service vehicles in a garage or during testing of new software. The drawback of these protocols is that when active they change the behaviour of the CAN bus. A test of the system while a debugging stream is active is thus an invalid test as disabling the stream alters the timing of the system. Other drawbacks of these protocols is the overhead, a maximum size CAN frame already has 44 bits of overhead for every 64 bits of data in the best case before stuffing. The XCP, CCP and UDS protocols add extra headers in the data, reducing effective data transmission further. When considering a TSN based network this kind of traffic could either be mapped to best-effort traffic for non-time critical streams such as a software update. Or could be scheduled using the Credit Based Shaper for debugging data where a certain bandwidth is necessary, but the exact arrival times are not critical.

We have seen that the data dictionaries are generated and consumed at fixed rates. Making it suitable for transmission using the Time Aware Shaper in a TSN network. Usage of the Time Aware Shaper would remove the impact of runnables timing characteristics on the system by specifying a strict schedule. As a downside it reduces flexibility in the deployment of runnables. But since an Ethernet frame can contain 1500 bytes of payload all data dictionaries can be contained in just two Ethernet frames, one strategy could be to broadcast every data dictionary to every programmable end node at the maximum production rate. This would roughly equate to 208 kbit/second of payload data assuming a rate of 10 ms. Which is far lower than the 100 mbit/s available in Ethernet networks. An added benefit is the synchronization between all the system variables, removing bugs caused by communication delays. The ability to order the system's data using the globally accurate and synchronized time base provided by TSN greatly improves testing and debugging as well.

The model deviates from the implemented system as it assumes that runnables are scheduled by the real-time operating system. While in reality the runnables of a certain rate have been grouped together in one task that is scheduled. No reasoning could be found for the exact ordering of the runnables inside a task. Searching through the source control no evidence was found that the orderings where changed in order to optimize some performance. Our assumption is that they are somewhat arbitrary. The gap in documentation regarding scheduling led us to decide that the scheduler selects a random runnable from the available runnables for execution. In reality this process would likely be deterministic. But since the experiments are only a demonstration of the model's capabilities the influence on the results is not relevant. Similarly, no concrete data on the execution time of the tasks is retrieved. During the experiments we noted that the Vehicle Control Unit was overloaded, resulting in dropped CAN messages and a large spread in the age of data dictionary reads. This result should not be seen as proof of an issue in the real system. But we have seen that the Vehicle Control Unit receives a lot of CAN messages and that it may lead to spend a significant of time reading those messages. Which can be attributed to the event based nature of the receive task. Demonstrating that the model can be used to find bottlenecks in the system. Further analysis of a realistic ordering and execution times will improve the accuracy of the model without impacting the ability to answer relevant performance questions.

The view of a system's state can vary in a decentralized system as data must be transmitted to the various nodes. This difference in system state is undesirable and can lead to issues such as controller instability. Our model records the difference in time between the generation of the sample and the time it is used. This metric can be used by system designers to determine whether the network performance supports the application requirements. We tried incorporating the age of input data dictionaries into the age of a data dictionary sample. The reason being that an old sample influences the calculations resulting in new samples of other data dictionaries. Unfortunately this was unsuccessful as no relationship between data dictionaries was defined in a structured way.

The work has mainly focussed in modelling a CAN based architecture and creating a simulation framework that can be used to evaluate the effects of network configuration on application performance. The automotive industry proposes a two-step transition from the traditional functional CAN based architecture to the zonal TSN based architecture. The first step introduces an TSN based backbone used by traditional domain based, or zonal controllers to communicate with each other. The domain based and zonal controllers act as a gateway between the TSN backbone and the legacy networks used to communicate with the controllers that still use CAN or LIN as their primary communication network. In the second step the controllers in a physical zone are consolidated into a single zonal ECU and a central controller is added resulting in the architecture depicted in figure~\ref{fig:zonal-arch}. We have seen that there are three major controllers in Lightyear's architecture. The application software already decouples the exchange of data between runnables from the transmission medium. A CAN receive and transmit runnable are automatically generated from the data dictionary interface description. A change to runnables receiving and transmitting TSN frames would be transparent to the runnables. A benefit would be that all three nodes can communicate directly with each other instead of the bridging required in the current architecture. But a decision must be made regarding the Vehicle and Powertrain CAN busses. In the current architecture they are used by several parametrizable end nodes as well as the programmable end nodes. If necessary the current CAN connection between programmable end nodes could be kept next to the Ethernet connection, allowing both nodes to communicate to the parametrizable end nodes. Or the busses are split up as to isolate the nodes into the required domains. Further investigation is necessary to determine the best course of action. The presented model accurately represents the software architecture of the programmable end nodes. The CAN receive and transmit runnables could be modified into TSN based runnables that use a TSN interface from the INET model suite. Which would be transparent for the other runnables and data dictionaries.